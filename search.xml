<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[再谈IO多路复用之epoll]]></title>
    <url>%2F2019%2F05%2F12%2Flinux.io.multiplexing.epoll%2F</url>
    <content type="text"><![CDATA[APIepoll_create123#include &lt;sys/epoll.h&gt;int epoll_create(int size); 创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 12345lrwx------. 1 root root 64 5月 12 11:45 0 -&gt; /dev/pts/0lrwx------. 1 root root 64 5月 12 11:45 1 -&gt; /dev/pts/0lrwx------. 1 root root 64 5月 12 11:45 2 -&gt; /dev/pts/2lrwx------. 1 root root 64 5月 12 11:45 3 -&gt; socket:[251991]lrwx------. 1 root root 64 5月 12 11:45 4 -&gt; anon_inode:[eventpoll] size：告诉内核这个监听的数目一共有多大。 epoll_ctl123#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。 epfd：epoll_create()的返回值。 op：动作，用三个宏来表示： EPOLL_CTL_ADD：注册新的fd到epfd中。 EPOLL_CTL_MOD：修改已经注册的fd的监听事件。 EPOLL_CTL_DEL：从epfd中删除一个fd。 fd：需要监听的fd。 event：告诉内核需要监听什么事，struct epoll_event结构如下： 1234struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;; events可以是以下几个宏的集合： EPOLLIN：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）。 EPOLLOUT：表示对应的文件描述符可以写。 EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）。 EPOLLERR：表示对应的文件描述符发生错误。 EPOLLHUP：表示对应的文件描述符被挂断。 EPOLLET：将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。 LT模式：默认为此模式，当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 epoll_wait123#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 等待事件的产生，类似于select()调用。 epfd：epoll_create()的返回值。 events：从内核得到事件的集合。 maxevents：告诉内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size。 timeout：超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 返回值：需要处理的事件数目，如返回0表示已超时，-1表示有错误发生。 特点epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 Example下面的程序是基于socket的tcp应答程序。 服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define BACKLOG 5 //完成三次握手但没有accept的队列的长度#define CONCURRENT_MAX 8 //应用层同时可以处理的连接#define SERVER_PORT 11332#define BUFFER_SIZE 1024#define QUIT_CMD ".quit\n"int main(int argc, const char * argv[])&#123; char input_msg[BUFFER_SIZE]; char recv_msg[BUFFER_SIZE]; //本地地址 struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(SERVER_PORT); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); //创建socket int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; //绑定socket int bind_result = bind(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); if(bind_result == -1) &#123; perror("bind error"); return 1; &#125; //listen if(listen(server_sock_fd, BACKLOG) == -1) &#123; perror("listen error"); return 1; &#125; //create epollfd int events_len = CONCURRENT_MAX + 2; int epollfd = epoll_create(events_len); if(epollfd == -1) &#123; fprintf(stderr, "create epollfd failed!\n"); exit(-1); &#125; //clientfd int clientfds[CONCURRENT_MAX]; for(int i = 0;i &lt; CONCURRENT_MAX;i++) &#123; clientfds[i] = -1; &#125; //add event to kernel struct epoll_event stdin_event; stdin_event.events = EPOLLIN; stdin_event.data.fd = STDIN_FILENO; epoll_ctl(epollfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;stdin_event); struct epoll_event server_event; server_event.events = EPOLLIN; server_event.data.fd = server_sock_fd; epoll_ctl(epollfd, EPOLL_CTL_ADD, server_sock_fd, &amp;server_event); //get events from kernel int timeout = 20 * 1000; struct epoll_event events[events_len]; //do epoll while(1) &#123; int ret = epoll_wait(epollfd, events, events_len, timeout); if(ret &lt; 0) &#123; perror("epoll 出错\n"); continue; &#125; else if(ret == 0) &#123; printf("epoll 超时\n"); continue; &#125; else &#123; for(int i = 0;i &lt; ret;i++) &#123; int fd = events-&gt;data.fd; if(fd == server_sock_fd &amp;&amp; (events-&gt;events &amp; server_event.events)) &#123; //有新的连接请求 struct sockaddr_in client_address; socklen_t address_len; int client_sock_fd = accept(server_sock_fd, (struct sockaddr *)&amp;client_address, &amp;address_len); printf("new connection client_sock_fd = %d\n", client_sock_fd); if(client_sock_fd &gt; 0) &#123; int index = -1; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(clientfds[client_i] == -1) &#123; index = client_i; clientfds[client_i] = client_sock_fd; // add event to kernel struct epoll_event client_event; client_event.events = EPOLLIN; client_event.data.fd = client_sock_fd; epoll_ctl(epollfd, EPOLL_CTL_ADD, client_sock_fd, &amp;client_event); break; &#125; &#125; if(index &gt;= 0) &#123; printf("新客户端(%d)加入成功 %s:%d\n", index, inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; else &#123; bzero(input_msg, BUFFER_SIZE); strcpy(input_msg, "服务器加入的客户端数达到最大值,无法加入!\n"); send(client_sock_fd, input_msg, BUFFER_SIZE, 0); printf("客户端连接数达到最大值，新客户端加入失败 %s:%d\n", inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; &#125; &#125; else if(fd == STDIN_FILENO &amp;&amp; (events-&gt;events &amp; stdin_event.events)) &#123; bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); //输入“.quit"则退出服务器 if(strcmp(input_msg, QUIT_CMD) == 0) &#123; exit(0); &#125; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(clientfds[client_i] &gt; 0) &#123; printf("向客户端(%d)发送消息\n", client_i); send(clientfds[client_i], input_msg, BUFFER_SIZE, 0); &#125; &#125; &#125; else &#123; //处理某个客户端过来的消息 bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(events[i].data.fd, recv_msg, BUFFER_SIZE, 0); if(byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(clientfds[client_i] == events[i].data.fd) &#123; printf("客户端(%d):%s\n", client_i, recv_msg); break; &#125; &#125; &#125; else if(byte_num &lt; 0) &#123; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(clientfds[client_i] == events[i].data.fd) &#123; printf("从客户端(%d)接受消息出错.\n", client_i); break; &#125; &#125; &#125; else &#123; // delete event in kernel struct epoll_event client_event; client_event.events = EPOLLIN; client_event.data.fd = events[i].data.fd; epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, &amp;client_event); for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(clientfds[client_i] == events[i].data.fd) &#123; clientfds[client_i] = -1; printf("客户端(%d)退出了.\n", client_i); break; &#125; &#125; &#125; &#125; &#125; &#125; &#125; close(epollfd); return 0;&#125; 客户端（仍然采用select）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;string.h&gt;#include&lt;unistd.h&gt;#define BUFFER_SIZE 1024int main(int argc, const char * argv[])&#123; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(11332); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; char recv_msg[BUFFER_SIZE]; char input_msg[BUFFER_SIZE]; if(connect(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(struct sockaddr_in)) == 0) &#123; fd_set client_fd_set; struct timeval tv; while(1) &#123; tv.tv_sec = 20; tv.tv_usec = 0; FD_ZERO(&amp;client_fd_set); FD_SET(STDIN_FILENO, &amp;client_fd_set); FD_SET(server_sock_fd, &amp;client_fd_set); select(server_sock_fd + 1, &amp;client_fd_set, NULL, NULL, &amp;tv); if(FD_ISSET(STDIN_FILENO, &amp;client_fd_set)) &#123; bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); if(send(server_sock_fd, input_msg, BUFFER_SIZE, 0) == -1) &#123; perror("发送消息出错!\n"); &#125; &#125; if(FD_ISSET(server_sock_fd, &amp;client_fd_set)) &#123; bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(server_sock_fd, recv_msg, BUFFER_SIZE, 0); if(byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; printf("服务器:%s\n", recv_msg); &#125; else if(byte_num &lt; 0) &#123; printf("接受消息出错!\n"); &#125; else &#123; printf("服务器端退出!\n"); exit(0); &#125; &#125; &#125; //&#125; &#125; return 0;&#125; 参考 http://www.cnblogs.com/Anker/archive/2013/08/17/3263780.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[再谈IO多路复用之poll]]></title>
    <url>%2F2019%2F05%2F09%2Flinux.io.multiplexing.poll%2F</url>
    <content type="text"><![CDATA[API123#include &lt;poll.h&gt;int poll( struct pollfd *fds, unsigned int nfds, int timeout); fds：待测试的描述以及待测试的事件等，可以是多个。pollfd结构体如下： 12345struct pollfd &#123; int fd; /* 文件描述符 */ short events; /* 等待的事件 */ short revents; /* 实际发生了的事件 */&#125;; 每一个pollfd结构体指定了一个被监视的文件描述符，可以传递多个结构体，指示poll()监视多个文件描述符。每个结构体的events域是监视该文件描述符的事件掩码，由用户来设置这个域。revents域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域。events域中请求的任何事件都可能在revents域中返回。合法的事件如下： POLLIN：有数据可读 POLLRDNORM：有普通数据可读 POLLRDBAND：有优先数据可读 POLLPRI：有紧迫数据可读 POLLOUT：写数据不会导致阻塞 POLLWRNORM：写普通数据不会导致阻塞 POLLWRBAND：写优先数据不会导致阻塞 POLLMSGSIGPOLL：消息可用 此外，revents域中还可能返回下列事件： POLLER：指定的文件描述符发生错误 POLLHUP：指定的文件描述符挂起事件 POLLNVAL：指定的文件描述符非法 POLLIN | POLLPRI等价于select()的读事件，POLLOUT | POLLWRBAND等价于select()的写事件。POLLIN等价于POLLRDNORM | POLLRDBAND，而POLLOUT则等价于POLLWRNORM。例如，要同时监视一个文件描述符是否可读和可写，我们可以设置 events为POLLIN | POLLOUT。在poll返回时，我们可以检查revents中的标志，对应于文件描述符请求的events结构体。如果POLLIN事件被设置，则文件描述符可以被读取而不阻塞。如果POLLOUT被设置，则文件描述符可以写入而不导致阻塞。这些标志并不是互斥的：它们可能被同时设置，表示这个文件描述符的读取和写入操作都会正常返回而不阻塞。 nfds：fds的个数。 timeout：参数指定等待的毫秒数，无论I/O是否准备好，poll都会返回。timeout指定为负数值表示无限超时，使poll()一直挂起直到一个指定事件发生；timeout为0指示poll调用立即返回并列出准备好I/O的文件描述符，但并不等待其它的事件。这种情况下，poll()就像它的名字那样，一旦选举出来，立即返回。 返回值：成功时，poll()返回结构体中revents域不为0的文件描述符个数；如果在超时前没有任何事件发生，poll()返回0；失败时，poll()返回-1，并设置errno为下列值之一： EBADF：一个或多个结构体中指定的文件描述符无效 EFAULTfds：指针指向的地址超出进程的地址空间 EINTR：请求的事件之前产生一个信号，调用可以重新发起 EINVALnfds：参数超出PLIMIT_NOFILE值 ENOMEM：可用内存不足，无法完成请求 缺点poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。 Example下面的程序是基于socket的tcp应答程序。 服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;poll.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define BACKLOG 5 //完成三次握手但没有accept的队列的长度#define CONCURRENT_MAX 8 //应用层同时可以处理的连接#define SERVER_PORT 11332#define BUFFER_SIZE 1024#define QUIT_CMD ".quit\n"int main(int argc, const char * argv[])&#123; char input_msg[BUFFER_SIZE]; char recv_msg[BUFFER_SIZE]; //本地地址 struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(SERVER_PORT); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); //创建socket int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; //绑定socket int bind_result = bind(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); if(bind_result == -1) &#123; perror("bind error"); return 1; &#125; //listen if(listen(server_sock_fd, BACKLOG) == -1) &#123; perror("listen error"); return 1; &#125; //pollfd int timeout = 20 * 1000; int fds_len = 2 + CONCURRENT_MAX; struct pollfd fds[fds_len]; for(int i = 0;i &lt; fds_len;i++) &#123; fds[i].fd = -1; fds[i].events = POLLIN; fds[i].revents = 0; &#125; fds[0].fd = STDIN_FILENO; fds[1].fd = server_sock_fd; //do poll while(1) &#123; int ret = poll(fds, fds_len, timeout); if(ret &lt; 0) &#123; perror("poll 出错\n"); continue; &#125; else if(ret == 0) &#123; printf("poll 超时\n"); continue; &#125; else &#123; for(int i = 0;i &lt; fds_len;i++) &#123; if(fds[i].revents &amp; fds[i].events) &#123; fds[i].revents = 0; if(i == 0) &#123; printf("发送消息：\n"); bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); //输入“.quit"则退出服务器 if(strcmp(input_msg, QUIT_CMD) == 0) &#123; exit(0); &#125; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(fds[client_i + 2].fd &gt; 0) &#123; printf("向客户端(%d)发送消息\n", client_i); send(fds[client_i + 2].fd, input_msg, BUFFER_SIZE, 0); &#125; &#125; &#125; else if(i == 1) &#123; //有新的连接请求 struct sockaddr_in client_address; socklen_t address_len; int client_sock_fd = accept(server_sock_fd, (struct sockaddr *)&amp;client_address, &amp;address_len); printf("new connection client_sock_fd = %d\n", client_sock_fd); if(client_sock_fd &gt; 0) &#123; int index = -1; for(int client_i = 0;client_i &lt; CONCURRENT_MAX;client_i++) &#123; if(fds[client_i + 2].fd == -1) &#123; index = client_i; fds[client_i + 2].fd = client_sock_fd; break; &#125; &#125; if(index &gt;= 0) &#123; printf("新客户端(%d)加入成功 %s:%d\n", index, inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; else &#123; bzero(input_msg, BUFFER_SIZE); strcpy(input_msg, "服务器加入的客户端数达到最大值,无法加入!\n"); send(client_sock_fd, input_msg, BUFFER_SIZE, 0); printf("客户端连接数达到最大值，新客户端加入失败 %s:%d\n", inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; &#125; &#125; else &#123; //处理某个客户端过来的消息 bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(fds[i].fd, recv_msg, BUFFER_SIZE, 0); if(byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; printf("客户端(%d):%s\n", i - 2, recv_msg); &#125; else if(byte_num &lt; 0) &#123; printf("从客户端(%d)接受消息出错.\n", i - 2); &#125; else &#123; fds[i].fd = -1; fds[i].revents = 0; printf("客户端(%d)退出了\n", i - 2); &#125; &#125; &#125; &#125; &#125; &#125; return 0;&#125; 客户端（仍然采用select）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;string.h&gt;#include&lt;unistd.h&gt;#define BUFFER_SIZE 1024int main(int argc, const char * argv[])&#123; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(11332); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; char recv_msg[BUFFER_SIZE]; char input_msg[BUFFER_SIZE]; if(connect(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(struct sockaddr_in)) == 0) &#123; fd_set client_fd_set; struct timeval tv; while(1) &#123; tv.tv_sec = 20; tv.tv_usec = 0; FD_ZERO(&amp;client_fd_set); FD_SET(STDIN_FILENO, &amp;client_fd_set); FD_SET(server_sock_fd, &amp;client_fd_set); select(server_sock_fd + 1, &amp;client_fd_set, NULL, NULL, &amp;tv); if(FD_ISSET(STDIN_FILENO, &amp;client_fd_set)) &#123; bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); if(send(server_sock_fd, input_msg, BUFFER_SIZE, 0) == -1) &#123; perror("发送消息出错!\n"); &#125; &#125; if(FD_ISSET(server_sock_fd, &amp;client_fd_set)) &#123; bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(server_sock_fd, recv_msg, BUFFER_SIZE, 0); if(byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; printf("服务器:%s\n", recv_msg); &#125; else if(byte_num &lt; 0) &#123; printf("接受消息出错!\n"); &#125; else &#123; printf("服务器端退出!\n"); exit(0); &#125; &#125; &#125; //&#125; &#125; return 0;&#125; 参考 http://www.cnblogs.com/Anker/archive/2013/08/15/3261006.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[再谈IO多路复用之select]]></title>
    <url>%2F2019%2F05%2F06%2Flinux.io.multiplexing.select%2F</url>
    <content type="text"><![CDATA[API1234#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;int select(int maxfdp1, fd_set *readset, fd_set *writeset, fd_set *exceptset, const struct timeval *timeout); maxfdp1：待测试的描述符个数，最大描述符加1。因为描述符是从0开始的。 readset：让内核测试读的描述字，如果不敢兴趣则置为空指针。 writeset：让内核测试写的描述字，如果不敢兴趣则置为空指针。 exceptset：让内核测试异常的描述字，如果不敢兴趣则置为空指针。 timeout：告知内核等待所指定描述字中的任何一个就绪可花多少时间。这个参数有以下几种可能性： 永远等待下去：仅在有一个描述字准备好I/O时才返回。为此，把该参数设置为空指针NULL。 等待一段固定时间：在有一个描述字准备好I/O时返回，但是不超过由该参数所指向的timeval结构中指定的秒数和微秒数。 根本不等待：检查描述字后立即返回，这称为轮询。为此，该参数必须指向一个timeval结构，而且其中的定时器值必须为0。 timeval结构体如下： 1234struct timeval &#123; long tv_sec; //seconds long tv_usec; //microseconds&#125;; 缺点 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 select支持的文件描述符数量太小了，默认是1024 Example下面的程序是基于socket的tcp应答程序。 服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;string.h&gt;#include&lt;unistd.h&gt;#define BACKLOG 5 //完成三次握手但没有accept的队列的长度#define CONCURRENT_MAX 8 //应用层同时可以处理的连接#define SERVER_PORT 11332#define BUFFER_SIZE 1024#define QUIT_CMD ".quit\n"int client_fds[CONCURRENT_MAX];int main(int argc, const char * argv[])&#123; char input_msg[BUFFER_SIZE]; char recv_msg[BUFFER_SIZE]; //本地地址 struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(SERVER_PORT); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); //创建socket int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; //绑定socket int bind_result = bind(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); if(bind_result == -1) &#123; perror("bind error"); return 1; &#125; //listen if(listen(server_sock_fd, BACKLOG) == -1) &#123; perror("listen error"); return 1; &#125; //fd_set fd_set server_fd_set; int max_fd = -1; struct timeval tv; //超时时间设置 while(1) &#123; tv.tv_sec = 20; tv.tv_usec = 0; FD_ZERO(&amp;server_fd_set); FD_SET(STDIN_FILENO, &amp;server_fd_set); if(max_fd &lt;STDIN_FILENO) &#123; max_fd = STDIN_FILENO; &#125; //printf("STDIN_FILENO=%d\n", STDIN_FILENO); //服务器端socket FD_SET(server_sock_fd, &amp;server_fd_set); // printf("server_sock_fd=%d\n", server_sock_fd); if(max_fd &lt; server_sock_fd) &#123; max_fd = server_sock_fd; &#125; //客户端连接 for(int i =0; i &lt; CONCURRENT_MAX; i++) &#123; //printf("client_fds[%d]=%d\n", i, client_fds[i]); if(client_fds[i] != 0) &#123; FD_SET(client_fds[i], &amp;server_fd_set); if(max_fd &lt; client_fds[i]) &#123; max_fd = client_fds[i]; &#125; &#125; &#125; int ret = select(max_fd + 1, &amp;server_fd_set, NULL, NULL, &amp;tv); if(ret &lt; 0) &#123; perror("select 出错\n"); continue; &#125; else if(ret == 0) &#123; printf("select 超时\n"); continue; &#125; else &#123; //ret 为未状态发生变化的文件描述符的个数 if(FD_ISSET(STDIN_FILENO, &amp;server_fd_set)) &#123; printf("发送消息：\n"); bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); //输入“.quit"则退出服务器 if(strcmp(input_msg, QUIT_CMD) == 0) &#123; exit(0); &#125; for(int i = 0; i &lt; CONCURRENT_MAX; i++) &#123; if(client_fds[i] != 0) &#123; printf("client_fds[%d]=%d\n", i, client_fds[i]); send(client_fds[i], input_msg, BUFFER_SIZE, 0); &#125; &#125; &#125; if(FD_ISSET(server_sock_fd, &amp;server_fd_set)) &#123; //有新的连接请求 struct sockaddr_in client_address; socklen_t address_len; int client_sock_fd = accept(server_sock_fd, (struct sockaddr *)&amp;client_address, &amp;address_len); printf("new connection client_sock_fd = %d\n", client_sock_fd); if(client_sock_fd &gt; 0) &#123; int index = -1; for(int i = 0; i &lt; CONCURRENT_MAX; i++) &#123; if(client_fds[i] == 0) &#123; index = i; client_fds[i] = client_sock_fd; break; &#125; &#125; if(index &gt;= 0) &#123; printf("新客户端(%d)加入成功 %s:%d\n", index, inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; else &#123; bzero(input_msg, BUFFER_SIZE); strcpy(input_msg, "服务器加入的客户端数达到最大值,无法加入!\n"); send(client_sock_fd, input_msg, BUFFER_SIZE, 0); printf("客户端连接数达到最大值，新客户端加入失败 %s:%d\n", inet_ntoa(client_address.sin_addr), ntohs(client_address.sin_port)); &#125; &#125; &#125; for(int i =0; i &lt; CONCURRENT_MAX; i++) &#123; if(client_fds[i] !=0) &#123; if(FD_ISSET(client_fds[i], &amp;server_fd_set)) &#123; //处理某个客户端过来的消息 bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(client_fds[i], recv_msg, BUFFER_SIZE, 0); if (byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; printf("客户端(%d):%s\n", i, recv_msg); &#125; else if(byte_num &lt; 0) &#123; printf("从客户端(%d)接受消息出错.\n", i); &#125; else &#123; FD_CLR(client_fds[i], &amp;server_fd_set); client_fds[i] = 0; printf("客户端(%d)退出了\n", i); &#125; &#125; &#125; &#125; &#125; &#125; return 0;&#125; 客户端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;string.h&gt;#include&lt;unistd.h&gt;#define BUFFER_SIZE 1024int main(int argc, const char * argv[])&#123; struct sockaddr_in server_addr; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(11332); server_addr.sin_addr.s_addr = inet_addr("127.0.0.1"); bzero(&amp;(server_addr.sin_zero), 8); int server_sock_fd = socket(AF_INET, SOCK_STREAM, 0); if(server_sock_fd == -1) &#123; perror("socket error"); return 1; &#125; char recv_msg[BUFFER_SIZE]; char input_msg[BUFFER_SIZE]; if(connect(server_sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(struct sockaddr_in)) == 0) &#123; fd_set client_fd_set; struct timeval tv; while(1) &#123; tv.tv_sec = 20; tv.tv_usec = 0; FD_ZERO(&amp;client_fd_set); FD_SET(STDIN_FILENO, &amp;client_fd_set); FD_SET(server_sock_fd, &amp;client_fd_set); select(server_sock_fd + 1, &amp;client_fd_set, NULL, NULL, &amp;tv); if(FD_ISSET(STDIN_FILENO, &amp;client_fd_set)) &#123; bzero(input_msg, BUFFER_SIZE); fgets(input_msg, BUFFER_SIZE, stdin); if(send(server_sock_fd, input_msg, BUFFER_SIZE, 0) == -1) &#123; perror("发送消息出错!\n"); &#125; &#125; if(FD_ISSET(server_sock_fd, &amp;client_fd_set)) &#123; bzero(recv_msg, BUFFER_SIZE); long byte_num = recv(server_sock_fd, recv_msg, BUFFER_SIZE, 0); if(byte_num &gt; 0) &#123; if(byte_num &gt; BUFFER_SIZE) &#123; byte_num = BUFFER_SIZE; &#125; recv_msg[byte_num] = '\0'; printf("服务器:%s\n", recv_msg); &#125; else if(byte_num &lt; 0) &#123; printf("接受消息出错!\n"); &#125; else &#123; printf("服务器端退出!\n"); exit(0); &#125; &#125; &#125; //&#125; &#125; return 0;&#125; 参考 http://www.cnblogs.com/Anker/p/3265058.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[同步IO、异步IO、阻塞IO、非阻塞IO]]></title>
    <url>%2F2019%2F05%2F05%2Flinux.io%2F</url>
    <content type="text"><![CDATA[概念剖析 同步所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件事一件事地做，等前一件做完了才能做下一件事。例如B/S模式（同步）：提交请求-&gt;等待服务器处理-&gt;处理完毕返回，这个期间客户端浏览器不能干任何事。 异步异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。例如 ajax请求（异步）: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕。 阻塞阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回,它还会抢占cpu去执行其他逻辑，也会主动检测io是否准备好。 非阻塞非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 IO模型 阻塞I/O（blocking I/O）使用recv的默认参数一直等数据直到拷贝到用户空间，这段时间内进程始终阻塞。 非阻塞I/O（nonblocking I/O）改变flags，让recv不管有没有获取到数据都返回，如果没有数据那么一段时间后再调用recv看看，如此循环。但是它只有检查有无数据的时候是非阻塞的，在数据到达的时候依然要等待复制数据到用户空间(等着水将水杯装满)，因此它还是同步IO。 IO复用（select，poll和epoll）这里在调用recv前先调用select或者poll，这2个系统调用都可以在内核准备好数据（网络数据到达内核）时告知用户进程，这个时候再调用recv一定是有数据的。因此这一过程中它是阻塞于select或poll，而没有阻塞于recv，有人将非阻塞IO定义成在读写操作时没有阻塞于系统调用的IO操作(不包括数据从内核复制到用户空间时的阻塞，因为这相对于网络IO来说确实很短暂)，如果按这样理解，这种IO模型也能称之为非阻塞IO模型，但是按POSIX来看，它也是同步IO。 信号驱动IO通过调用sigaction注册信号函数，等内核数据准备好的时候系统中断当前程序，执行信号函数(在这里面调用recv)。 异步IO调用aio_read，让内核等数据准备好，并且复制到用户进程空间后执行事先指定好的函数。 总结 数据准备阶段 内核空间复制回用户空间阻塞IO模型、非阻塞IO模型、IO复用模型(select/poll/epoll)、信号驱动IO模型都属于同步IO，因为阶段2是阻塞的（尽管时间很短）。只有异步IO模型是真真正正的异步IO，因为不管在阶段1还是阶段2都可以干别的事。 参考 https://www.cnblogs.com/chaser24/p/6112071.htmlhttps://www.cnblogs.com/euphie/p/6376508.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux的用户态和内核态]]></title>
    <url>%2F2019%2F05%2F05%2Flinux.user.kernel%2F</url>
    <content type="text"><![CDATA[Linux体系架构从宏观上来看，Linux操作系统的体系架构分为用户态和内核态（或者用户空间和内核空间）。内核从本质上看是一种软件（控制计算机的硬件资源，并提供上层应用程序运行的环境）。用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括CPU资源、存储资源、I/O资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。 系统调用是操作系统的最小功能单位，这些系统调用根据不同的应用场景可以进行扩展和裁剪，现在各种版本的Unix实现都提供了不同数量的系统调用，如Linux的不同版本提供了240-260个系统调用，FreeBSD大约提供了320个（reference：UNIX环境高级编程）。我们可以把系统调用看成是一种不能再化简的操作（类似于原子操作，但是不同概念），有人把它比作一个汉字的一个“笔画”，而一个“汉字”就代表一个上层应用，我觉得这个比喻非常贴切。因此，有时候如果要实现一个完整的汉字（给某个变量分配内存空间），就必须调用很多的系统调用。如果从实现者（程序员）的角度来看，这势必会加重程序员的负担，良好的程序设计方法是：重视上层的业务逻辑操作，而尽可能避免底层复杂的实现细节。库函数正是为了将程序员从复杂的细节中解脱出来而提出的一种有效方法。它实现对系统调用的封装，将简单的业务逻辑接口呈现给用户，方便用户调用，从这个角度上看，库函数就像是组成汉字的“偏旁”。这样的一种组成方式极大增强了程序设计的灵活性，对于简单的操作，我们可以直接调用系统调用来访问资源，如“人”，对于复杂操作，我们借助于库函数来实现，如“仁”。显然，这样的库函数依据不同的标准也可以有不同的实现版本，如ISO C 标准库，POSIX标准库等。 Shell是一个特殊的应用程序，俗称命令行，本质上是一个命令解释器，它下通系统调用，上通各种应用，通常充当着一种“胶水”的角色，来连接各个小功能程序，让不同程序能够以一个清晰的接口协同工作，从而增强各个程序的功能。同时，Shell是可编程的，它可以执行符合Shell语法的文本，这样的文本称为Shell脚本，通常短短的几行Shell脚本就可以实现一个非常大的功能，原因就是这些Shell语句通常都对系统调用做了一层封装。为了方便用户和系统交互，一般，一个Shell对应一个终端，终端是一个硬件设备，呈现给用户的是一个图形化窗口。我们可以通过这个窗口输入或者输出文本。这个文本直接传递给shell进行分析解释，然后执行。 下图是对上图的一个细分结构，从这个图上可以更进一步对内核所做的事有一个“全景式”的印象。主要表现为：向下控制硬件资源，向内管理操作系统资源：包括进程的调度和管理、内存的管理、文件系统的管理、设备驱动程序的管理以及网络资源的管理，向上则向应用程序提供系统调用的接口。从整体上来看，整个操作系统分为两层：用户态和内核态，这种分层的架构极大地提高了资源管理的可扩展性和灵活性，而且方便用户对资源的调用和集中式的管理，带来一定的安全性。 用户态和内核态的切换因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。所以，为了减少有限资源的访问和使用冲突，Unix/Linux的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel的X86架构的CPU提供了0到3四个特权级，数字越小，特权越高，Linux操作系统中主要采用了0和3两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如C函数库中的内存分配函数malloc()，它具体是使用sbrk()系统调用来分配内存，当malloc调用sbrk()的时候就涉及一次从用户态到内核态的切换，类似的函数还有printf()，调用的是wirte()系统调用来输出字符串，等等。 用户态的应用程序可以通过以下三种方式来访问内核态的资源： 系统调用 库函数 Shell脚本 到底在什么情况下会发生从用户态到内核态的切换，一般存在以下三种情况： 系统调用，原因如上分析 异常事件： 当CPU正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常。 外围设备的中断：当外围设备完成用户的请求操作后，会像CPU发出中断信号，此时，CPU就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。 系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断，这是操作系统为用户特别开放的一种中断，如Linux int 80h中断。所以，从触发方式和效果上来看，这三种切换方式是完全一样的，都相当于是执行了一个中断响应的过程。但是从触发的对象来看，系统调用是进程主动请求切换的，而异常和硬中断则是被动的。 总结本文仅是从宏观的角度去理解Linux用户态和内核态的设计，并没有去深究它们的具体实现方式。从实现上来看，必须要考虑到的一点我想就是性能问题，因为用户态和内核态之间的切换也会消耗大量资源。 参考 https://www.cnblogs.com/bakari/p/5520860.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[更换centos的yum源]]></title>
    <url>%2F2019%2F05%2F01%2Fcentos.yum%2F</url>
    <content type="text"><![CDATA[网上给出的将centos的yum源替换为阿里云源，目前不能用了，所以贴出下面源配置 centos5123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the# remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Base - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=1gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-5#released updates[updates]name=CentOS-$releasever - Updates - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-5#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=1gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-5#packages used/produced in the build but not released[addons]name=CentOS-$releasever - Addons - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/addons/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=addonsgpgcheck=1gpgkey=https://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-5#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=1enabled=0gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-5#contrib - packages by Centos Users[contrib]name=CentOS-$releasever - Contrib - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/contrib/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contribgpgcheck=1enabled=0gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-5 centos612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the# remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Base - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=1gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-6#released updates[updates]name=CentOS-$releasever - Updates - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=1gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-6#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=1gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-6#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=1enabled=0gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-6#contrib - packages by Centos Users[contrib]name=CentOS-$releasever - Contrib - mirrors.ustc.edu.cnbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/contrib/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contribgpgcheck=1enabled=0gpgkey=https://mirrors.ustc.edu.cn/centos/RPM-GPG-KEY-CentOS-6 centos712345678910111213141516171819202122232425262728293031323334353637383940414243# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the# remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Base#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$releasever - Updates# mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extras# mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus# mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusbaseurl=https://mirrors.ustc.edu.cn/centos/$releasever/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7]]></content>
  </entry>
  <entry>
    <title><![CDATA[字节序]]></title>
    <url>%2F2018%2F04%2F15%2Fsort.bytes%2F</url>
    <content type="text"><![CDATA[计算机硬件有两种字节序： 大端字节序（big endian）高位字节在前，低位字节在后。0x12345678表示为12 34 56 78。 小端字节序（little endian）低位字节在前，高位字节在后。0x12345678表示为78 56 34 12。 字节序的应用： 主机字节序（HBO，Host Byte Order）不同的机器HBO不相同，与CPU设计有关，数据的顺序是由cpu决定的,而与操作系统无关。 网络字节序（NBO，Network Byte Order）为了兼容性统一采取小端字节序，关于为什么会采用小端字节序而不是大端字节序，有这种说法，早期的网络非常不稳定，太容易丢失数据，为了尽量少地丢数据而采用小端字节序，因为即使丢了一部分数据那也只是丢失的低字节的数据^_^ 网络字节顺序与本地字节顺序之间的转换函数： htonlHost to Network Long ntohlNetwork to Host Long htonsHost to Network Short ntohsNetwork to Host Short 简单的查看本机字节序的代码： 第一段是看本机字节序，第二段是看网络字节序也就是小端字节序。 123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;int main()&#123; int i_num = 0x12345678; printf("[0]:0x%x\n", *((char *)&amp;i_num + 0)); printf("[1]:0x%x\n", *((char *)&amp;i_num + 1)); printf("[2]:0x%x\n", *((char *)&amp;i_num + 2)); printf("[3]:0x%x\n", *((char *)&amp;i_num + 3)); i_num = htonl(i_num); printf("[0]:0x%x\n", *((char *)&amp;i_num + 0)); printf("[1]:0x%x\n", *((char *)&amp;i_num + 1)); printf("[2]:0x%x\n", *((char *)&amp;i_num + 2)); printf("[3]:0x%x\n", *((char *)&amp;i_num + 3)); return 0;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux开机自启]]></title>
    <url>%2F2018%2F02%2F07%2Flinux.autostart%2F</url>
    <content type="text"><![CDATA[/etc/rc.local 执行的程序需要写绝对路径。 shell脚本 将写好的脚本（.sh文件）放到目录 /etc/profile.d/ 下，系统启动后就会自动执行该目录下的所有shell脚本。 chkconfig 将启动文件cp到 /etc/init.d/或者/etc/rc.d/init.d/（前者是后者的软连接）下，vim 编辑文件，编辑完成后执行chkconfig --add 脚本文件名 操作后就已经添加了，文件前面务必添加如下三行代码，否侧会提示chkconfig不支持： #!/bin/sh // 告诉系统使用的shell,所以的shell脚本都是这样 #chkconfig: 35 20 80 // 分别代表运行级别，启动优先权，关闭优先权，此行代码必须 #description: http server //（自己随便发挥），此行代码必须]]></content>
  </entry>
  <entry>
    <title><![CDATA[web相关]]></title>
    <url>%2F2017%2F11%2F30%2Fweb%2F</url>
    <content type="text"><![CDATA[HTTP Content-type HTML转义字符 RGB颜色参考 ASCII对照表 HTTP状态码详解 运算符优先级 TCP/UDP常见端口参考 网页字体参考 http://tool.oschina.net/commons]]></content>
  </entry>
  <entry>
    <title><![CDATA[windows或者linux查看本机的公网ip]]></title>
    <url>%2F2017%2F11%2F10%2Flinux.windows.ip%2F</url>
    <content type="text"><![CDATA[windowshttp://www.net.cn/static/customercare/yourip.asp linux1curl members.3322.org/dyndns/getip]]></content>
  </entry>
  <entry>
    <title><![CDATA[修改linux系统时间而不用重启]]></title>
    <url>%2F2017%2F10%2F20%2Flinux.change.time%2F</url>
    <content type="text"><![CDATA[查看时间是否正确，若不正确再修改 查看目前本地的时间 1date 查看硬件的时间 1hwclock --show 如果硬件时间和系统时间不同，那就对硬件的时间进行修改 修改时区 1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 设置硬件时间为14年12月15日15点15分15秒 1hwclock --set --date &apos;2017-04-17 14:44:15&apos; 设置系统时间和硬件时间同步 1hwclock --hctosys 保存时钟 1clock -w]]></content>
  </entry>
  <entry>
    <title><![CDATA[修改nginx配置文件允许跨域访问]]></title>
    <url>%2F2017%2F10%2F09%2Fnginx.conf.domain%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829location / &#123; if ($request_method = &apos;OPTIONS&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; # # Custom headers and headers various browsers *should* be OK with but aren&apos;t # add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; # # Tell client that this pre-flight info is valid for 20 days # add_header &apos;Access-Control-Max-Age&apos; 1728000; add_header &apos;Content-Type&apos; &apos;text/plain charset=UTF-8&apos;; add_header &apos;Content-Length&apos; 0; return 204; &#125; if ($request_method = &apos;POST&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; if ($request_method = &apos;GET&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; root /home/data; index index.html index.htm;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[nginx + spawn-fastcgi + fcgi]]></title>
    <url>%2F2017%2F10%2F08%2Ffastcgi%2F</url>
    <content type="text"><![CDATA[以下配置应该针对自己环境进行相应改变 nginx安装略过。nginx.conf配置如下，其中request_method配置是为了允许跨域访问： 12345678910111213141516171819202122232425262728location ~ modelsection\.cgi$ &#123; if ($request_method = &apos;OPTIONS&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; # # Tell client that this pre-flight info is valid for 20 days # add_header &apos;Access-Control-Max-Age&apos; 1728000; add_header &apos;Content-Type&apos; &apos;text/plain charset=UTF-8&apos;; add_header &apos;Content-Length&apos; 0; return 204; &#125; if ($request_method = &apos;POST&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; if ($request_method = &apos;GET&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; fastcgi_pass 127.0.0.1:9003; fastcgi_index index.cgi; fastcgi_param SCRIPT_FILENAME fcgi$fastcgi_script_name; include fastcgi.conf;&#125; spawn-fastcgi下载地址http://www.lighttpd.net/download/spawn-fcgi-1.6.2.tar.gz编译略过 fastcgi下载地址https://download.csdn.net/download/wangkangluo1/3268722编译略过 运行命令：spawn-fcgi -a 127.0.0.1 -p 9003 -f ./modelsection 参考地址：https://blog.csdn.net/ygm_linux/article/details/51261288]]></content>
  </entry>
  <entry>
    <title><![CDATA[TCP状态详解_2]]></title>
    <url>%2F2017%2F09%2F20%2Ftcp.2%2F</url>
    <content type="text"><![CDATA[TCP是一个面向连接的协议，所以在连接双方发送数据之前，都需要首先建立一条连接。这和前面讲到的协议完全不同。前面讲的所有协议都只是发送数据而已，大多数都不关心发送的数据是不是送到，UDP尤其明显，从编程的角度来说，UDP编程也要简单的多—-UDP都不用考虑数据分片。 书中用telnet登陆退出来解释TCP协议连接的建立和中止的过程，可以看到，TCP连接的建立可以简单的称为三次握手，而连接的中止则可以叫做四次握手。 连接的建立在建立连接的时候，客户端首先向服务器申请打开某一个端口(用SYN段等于1的TCP报文)，然后服务器端发回一个ACK报文通知客户端请求报文收到，客户端收到确认报文以后再次发出确认报文确认刚才服务器端发出的确认报文（绕口么），至此，连接的建立完成。这就叫做三次握手。如果打算让双方都做好准备的话，一定要发送三次报文，而且只需要三次报文就可以了。可以想见，如果再加上TCP的超时重传机制，那么TCP就完全可以保证一个数据包被送到目的地。 结束连接TCP有一个特别的概念叫做half-close，这个概念是说，TCP的连接是全双工（可以同时发送和接收）连接，因此在关闭连接的时候，必须关闭传和送两个方向上的连接。客户机给服务器一个FIN为1的TCP报文，然后服务器返回给客户端一个确认ACK报文，并且发送一个FIN报文，当客户机回复ACK报文后（四次握手），连接就结束了。 最大报文长度在建立连接的时候，通信的双方要互相确认对方的最大报文长度(MSS)，以便通信。一般这个SYN长度是MTU减去固定IP首部和TCP首部长度。对于一个以太网，一般可以达到1460字节。当然如果对于非本地的IP，这个MSS可能就只有536字节，而且，如果中间的传输网络的MSS更小的话，这个值还会变得更小。]]></content>
  </entry>
  <entry>
    <title><![CDATA[TCP状态详解_1]]></title>
    <url>%2F2017%2F09%2F20%2Ftcp.1%2F</url>
    <content type="text"><![CDATA[linux查看tcp的状态命令 netstat -nat查看TCP各个状态的数量 lsof -i:8080可以检测到打开套接字的状况 sar -n SOCK查看tcp创建的连接数 tcpdump -iany tcp port 9000对tcp端口为9000的进行抓包 tcp各个状态 LISTENING侦听来自远方的TCP端口的连接请求.首先服务端需要打开一个socket进行监听，状态为LISTEN。有提供某种服务才会处于LISTENING状态，TCP状态变化就是某个端口的状态变化，提供一个服务就打开一个端口，例如：提供www服务默认开的是80端口，提供ftp服务默认的端口为21，当提供的服务没有被连接时就处于LISTENING状态。FTP服务启动后首先处于侦听（LISTENING）状态。处于侦听LISTENING状态时，该端口是开放的，等待连接，但还没有被连接。就像你房子的门已经敞开的，但还没有人进来。 看LISTENING状态最主要的是看本机开了哪些端口，这些端口都是哪个程序开的，关闭不必要的端口是保证安全的一个非常重要的方面，服务端口都对应一个服务（应用程序），停止该服务就关闭了该端口，例如要关闭21端口只要停止IIS服务中的FTP服务即可。关于这方面的知识请参阅其它文章。如果你不幸中了服务端口的木马，木马也开个端口处于LISTENING状态。 SYN-SENT客户端SYN_SENT状态再发送连接请求后等待匹配的连接请求，客户端通过应用程序调用connect进行active open。于是客户端tcp发送一个SYN以请求建立一个连接。之后状态置为SYN_SENT。在发送连接请求后等待匹配的连接请求。当请求连接时客户端首先要发送同步信号给要访问的机器，此时状态为SYN_SENT，如果连接成功了就变为ESTABLISHED，正常情况下SYN_SENT状态非常短暂。例如要访问网站,如果是正常连接的话，用TCPView观察IEXPLORE.EXE（IE）建立的连接会发现很快从SYN_SENT变为ESTABLISHED，表示连接成功。SYN_SENT状态快的也许看不到。 如果发现有很多SYN_SENT出现，那一般有这么几种情况，一是你要访问的网站不存在或线路不好，二是用扫描软件扫描一个网段的机器，也会出现很多SYN_SENT，另外就是可能中了病毒了，例如中了”冲击波”，病毒发作时会扫描其它机器，这样会有很多SYN_SENT出现。 SYN-RECEIVED服务器端状态SYN_RCVD在收到和发送一个连接请求后等待对方对连接请求的确认。当服务器收到客户端发送的同步信号时，将标志位ACK和SYN置1发送给客户端，此时服务器端处于SYN_RCVD状态，如果连接成功了就变为ESTABLISHED，正常情况下SYN_RCVD状态非常短暂。 如果发现有很多SYN_RCVD状态，那你的机器有可能被SYN Flood的DoS(拒绝服务攻击)攻击了。SYN Flood的攻击原理是：在进行三次握手时，攻击软件向被攻击的服务器发送SYN连接请求（握手的第一步），但是这个地址是伪造的，如攻击软件随机伪造了51.133.163.104、65.158.99.152等等地址。服务器在收到连接请求时将标志位ACK和SYN置1发送给客户端（握手的第二步），但是这些客户端的IP地址都是伪造的，服务器根本找不到客户机，也就是说握手的第三步不可能完成。这种情况下服务器端一般会重试（再次发送SYN+ACK给客户端）并等待一段时间后丢弃这个未完成的连接，这段时间的长度我们称为SYN Timeout，一般来说这个时间是分钟的数量级（大约为30秒-2分钟）；一个用户出现异常导致服务器的一个线程等待1分钟并不是什么很大的问题，但如果有一个恶意的攻击者大量模拟这种情况，服务器端将为了维护一个非常大的半连接列表而消耗非常多的资源—-数以万计的半连接，即使是简单的保存并遍历也会消耗非常多的CPU时间和内存，何况还要不断对这个列表中的IP进行SYN+ACK的重试。此时从正常客户的角度看来，服务器失去响应，这种情况我们称做：服务器端受到了SYN Flood攻击（SYN洪水攻击）。 ESTABLISHED代表一个打开的连接ESTABLISHED状态是表示两台机器正在传输数据，观察这个状态最主要的就是看哪个程序正在处于ESTABLISHED状态。 服务器出现很多ESTABLISHED状态： netstat -nat |grep 9502或者使用lsof -i:9502可以检测到。当客户端未主动close的时候就断开连接，即客户端发送的FIN丢失或未发送。这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；结果客户端重新连接服务器。而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED; 如果客户端重复的上演这种情况，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。 FIN-WAIT-1等待远程TCP连接中断请求，或先前的连接中断请求的确认主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态。 如果服务器出现shutdown再重启，使用netstat -nat查看，就会看到很多FIN-WAIT-1的状态。就是因为服务器当前有很多客户端连接，直接关闭服务器后，无法接收到客户端的ACK。 FIN-WAIT-2从远程TCP等待连接中断请求主动关闭端接到ACK后，就进入了FIN-WAIT-2。 这就是著名的半关闭的状态了，这是在关闭连接时，客户端和服务器两次握手之后的状态。在这个状态下，应用程序还有接受数据的能力，但是已经无法发送数据，但是也有一种可能是，客户端一直处于FIN_WAIT_2状态，而服务器则一直处于WAIT_CLOSE状态，而直到应用层来决定关闭这个状态。 CLOSE-WAIT等待从本地用户发来的连接中断请求被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT。 CLOSING等待远程TCP对连接中断的确认比较少见 LAST-ACK等待原来的发向远程TCP的连接中断请求的确认被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接。这导致它的TCP也发送一个FIN，等待对方的ACK.就进入了LAST-ACK。 使用并发压力测试的时候，突然断开压力测试客户端，服务器会看到很多LAST-ACK。 TIME-WAIT等待足够的时间以确保远程TCP接收到连接中断请求的确认在主动关闭端接收到FIN后，TCP就发送ACK包，并进入TIME-WAIT状态。 TIME_WAIT等待状态，这个状态又叫做2MSL状态，说的是在TIME_WAIT2发送了最后一个ACK数据报以后，要进入TIME_WAIT状态，这个状态是防止最后一次握手的数据报没有传送到对方那里而准备的（注意这不是四次握手，这是第四次握手的保险状态）。这个状态在很大程度上保证了双方都可以正常结束，但是，问题也来了。由于插口的2MSL状态（插口是IP和端口对的意思，socket），使得应用程序在2MSL时间内是无法再次使用同一个插口的，对于客户程序还好一些，但是对于服务程序，例如httpd，它总是要使用同一个端口来进行服务，而在2MSL时间内，启动httpd就会出现错误（插口被使用）。为了避免这个错误，服务器给出了一个平静时间的概念，这是说在2MSL时间内，虽然可以重新启动服务器，但是这个服务器还是要平静的等待2MSL时间的过去才能进行下一次连接。 CLOSED没有任何连接状态被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束。 TCP状态迁移路线图 client/server两条路线讲述TCP状态迁移路线图：]]></content>
  </entry>
  <entry>
    <title><![CDATA[什么是2MSL]]></title>
    <url>%2F2017%2F09%2F19%2Fmsl%2F</url>
    <content type="text"><![CDATA[MSL是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为tcp报文（segment）是ip数据报（datagram）的数据部分，具体称谓请参见《数据在网络各层中的称呼》一文，而ip头中有一个TTL域，TTL是time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态，当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了TIME_WAIT状态，必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包。在TIME_WAIT状态时两端的端口不能使用，要等到2MSL时间结束才可继续使用。当连接处于2MSL等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置SO_REUSEADDR选项达到不必等待2MSL时间结束再使用此端口。 TTL与MSL是有关系的但不是简单的相等的关系，MSL要大于等于TTL。]]></content>
  </entry>
  <entry>
    <title><![CDATA[网络协议]]></title>
    <url>%2F2017%2F09%2F17%2Fosi.model%2F</url>
    <content type="text"><![CDATA[定义 网络协议为计算机网络中进行数据交换而建立的规则、标准或约定的集合。网络协议主要由三个要素组成： 语义，表示要做什么。 语法，表示要怎么做。 时序，表示做的顺序。 网络协议是网络上所有设备（网络服务器、计算机、交换机、路由器、防火墙等）之间通信规则的集合，它规定了通信时信息必须采用的格式和这些格式的意义。 由于网络节点之间联系的复杂性，在制定协议时，通常把复杂成分分解成一些简单成分，然后再将它们复合起来。最常用的复合技术就是层次方式，网络协议的层次结构如下： 结构中的每一层都规定有明确的服务及接口标准。 把用户的应用程序作为最高层。 除了最高层外，中间的每一层都向上一层提供服务，同时又是下一层的用户。 把物理通信线路作为最低层，它使用从最高层传送来的参数，是提供服务的基础。 分层详情 为了使不同计算机厂家生产的计算机能够相互通信，以便在更大的范围内建立计算机网络，国际标准化组织（ISO）在1978年提出了“开放系统互联参考模型”，即著名的OSI/RM模型（Open System Interconnection/Reference Model）。它将计算机网络体系结构的通信协议划分为七层，自下而上依次为：物理层（Physics Layer）、数据链路层（Data Link Layer）、网络层（Network Layer）、传输层（Transport Layer）、会话层（Session Layer）、表示层（Presentation Layer）、应用层（Application Layer）。其中第四层完成数据传送服务，上面三层面向用户。对于每一层，至少制定两项标准：服务定义和协议规范。前者给出了该层所提供的服务的准确定义，后者详细描述了该协议的动作和各种有关规程，以保证服务的提供。以下为各层详细介绍： 物理层 规定通信设备的机械的、电气的、功能的和过程的特性，用以建立、维护和拆除物理链路连接。具体地讲，机械 特性规定了网络连接时所需接插件的规格尺寸、引脚数量和排列情况等；电气特性规定了在物理连接上传输bit流时线路上信号电平的大小、阻抗匹配、传输速率 距离限制等；功能特性是指对各个信号先分配确切的信号含义，即定义了DTE和DCE之间各个线路的功能；规程特性定义了利用信号线进行bit流传输的一组操作规程。在这一层，数据的单位称为比特（bit）。属于物理层定义的包括：以太网、调制解调器、电力线通信(PLC)、SONET/SDH、G.709、光导纤维、同轴电缆、双绞线等。 数据链路层 在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路，通过差错控制提供数据帧（Frame）在信道上无差错的传输，并进行各电路上的动作系列。数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。在这一层，数据的单位称为帧（Frame）。属于数据链路层的包括：Wi-Fi(IEEE 802.11)、WiMAX(IEEE 802.16)、ATM、DTM、令牌环、以太网、FDDI、帧中继、GPRS、EVDO、HSPA、HDLC、PPP、L2TP、PPTP、ISDN、STP等。 网络层 在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。网络层将数据链路层提供的帧组成数据包，包中封装有网络层包头，其中含有逻辑地址信息–源站点和目的站点地址的网络地址。如果你在谈论一个IP地址，那么你是在处理第三层的问题，这是“数据包”问题，而不是第二层的“帧”。IP是第三层问题的一部分，此外还有一些路由协议和地址解析协议（ARP）。有关路由的一切事情都在这第三层处理。地址解析和路由是三层的重要目的。网络层还可以实现拥塞控制、网际互连等功能。在这一层，数据的单位称为数据包（packet）。网络层的协议包括：IP(IPv4、IPv6)、ICMP、ICMPv6、IGMP、IS-IS、IPsec、ARP、RARP等。 传输层 第4层的数据单元也称作数据包（packets）。但是，当你谈论TCP等具体的协议时又有特殊的叫法，TCP的数据单元称为段 （segments）而UDP协议的数据单元称为“数据报（datagrams）”。这个层负责获取全部信息，因此，它必须跟踪数据单元碎片、乱序到达的 数据包和其它在传输过程中可能发生的危险。第4层为上层提供端到端（最终用户到最终用户）的透明的、可靠的数据传输服务。所为透明的传输是指在通信过程中 传输层对上层屏蔽了通信传输系统的具体细节。传输层的协议包括：TCP、UDP、TLS、DCCP、SCTP、RSVP、OSPF等。 会话层 这一层也可以称为会晤层或对话层，在会话层及以上的高层次中，数据传送的单位不再另外命名，而是统称为报文。会话层不参与具体的传输，它提供包括访问验证和会话管理在内的建立和维护应用之间通信的机制。如服务器验证用户登录便是由会话层完成的。 表示层 这一层主要解决拥护信息的语法表示问题。它将欲交换的数据从适合于某一用户的抽象语法，转换为适合于OSI系统内部使用的传送语法。即提供格式化的表示和转换数据服务。数据的压缩和解压缩， 加密和解密等工作都由表示层负责。 应用层 应用层为操作系统或网络应用程序提供访问网络服务的接口。应用层的协议包括： DHCP、DNS、FTP、Gopher、HTTP、IMAP4、IRC、NNTP、XMPP、POP3、SIP、SMTP、SNMP、SSH、TELNET、RPC、RTCP、RTP、RTSP、SDP、SOAP、GTP、STUN、NTP、SSDP、BGP、RIP等。 概念及原理 Socket(套接字)套接字（socket）是通信的基石，是支持TCP/IP协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 TCP连接TCP连接包含三次握手过程，其过程介绍如下： 客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认； 服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态； 客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手”。 Socket连接由于通常情况下Socket连接就是TCP连接，因此Socket连接一旦建立，通信双方即可开始相互发送数据内容，直到双方连接断开。但在实际网络应用中，客户端到服务器之间的通信往往需要穿越多个中间节点，例如路由器、网关、防火墙等，大部分防火墙默认会关闭长时间处于非活跃状态的连接而导致 Socket连接断连，因此需要通过轮询告诉网络，该连接处于活跃状态。建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket。套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 服务器监听 服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 客户端请求 指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认 当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 HTTP连接HTTP协议是建立在TCP协议之上的一种应用，HTTP连接使用的是“请求—响应”的方式，不仅在请求时需要先建立连接，而且需要客户端向服务器发出请求后，服务器端才能回复数据。在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。通常的做法是即时不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道客户端“在线”。若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 TCP/IP协议，HTTP协议，Socket三者之间的关系实际上，传输层的TCP是基于网络层的IP协议的，而应用层的HTTP协议又是基于传输层的TCP协议的，而Socket本身不算是协议，它只是提供了一个针对TCP或者UDP编程的接口。]]></content>
  </entry>
  <entry>
    <title><![CDATA[正向代理和反向代理的理解]]></title>
    <url>%2F2017%2F09%2F16%2Fproxy.reverse%2F</url>
    <content type="text"><![CDATA[正向代理 我们常说的代理也就是只正向代理，正向代理的过程，它隐藏了真实的请求客户端，服务端不知道真实的客户端是谁，客户端请求的服务都被代理服务器代替来请求，某些科学上网工具扮演的就是典型的正向代理角色。用浏览器访问http://www.google.com时，被残忍的block，于是你可以在国外搭建一台代理服务器，让代理帮我去请求google.com，代理把请求返回的相应结构再返回给我。 反向代理 反向代理隐藏了真实的服务端，当我们请求http://www.baidu.com的时候，就像拨打10086一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，http://www.baidu.com就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到真实的服务器那里去。Nginx就是性能非常好的反向代理服务器，用来做负载均衡。 总结 两者的区别在于代理的对象不一样：正向代理代理的对象是客户端，反向代理代理的对象是服务端。]]></content>
  </entry>
  <entry>
    <title><![CDATA[用nginx的反向代理机制解决前端跨域问题]]></title>
    <url>%2F2017%2F09%2F16%2Fnginx.domain%2F</url>
    <content type="text"><![CDATA[什么是跨域以及产生原因 跨域是指a页面想获取b页面资源，如果a、b页面的协议、域名、端口、子域名不同，或是a页面为ip地址，b页面为域名地址，所进行的访问行动都是跨域的，而浏览器为了安全问题一般都限制了跨域访问，也就是不允许跨域请求资源。跨域情况如下： url 说明 是否跨域 http://www.cnblogs.com/a.jshttp://www.a.com/b.js 不同域名 是 http://www.a.com/lab/a.jshttp://www.a.com/script/b.js 同一域名下不同文件夹 否 http://www.a.com:8000/a.jshttp://www.a.com/b.js 同一域名，不同端口 是 http://www.a.com/a.jshttps://www.a.com/b.js 同一域名，不同协议 是 http://www.a.com/a.jshttp://70.32.92.74/b.js 域名和域名对应ip 是 http://www.a.com/a.jshttp://script.a.com/b.js 主域相同，子域不同 是 http://www.a.com/a.jshttp://a.com/b.js 同一域名，不同二级域名 是 跨域的常见解决方法 目前来讲没有不依靠服务器端来跨域请求资源的技术 jsonp 需要目标服务器配合一个callback函数。 window.name+iframe 需要目标服务器响应window.name。 window.location.hash+iframe 同样需要目标服务器作处理。 html5的postMessage+ifrme这个也是需要目标服务器或者说是目标页面写一个postMessage，主要侧重于前端通讯。 CORS需要服务器设置header ：Access-Control-Allow-Origin。(GZT使用的是这个方法) nginx反向代理 这个方法一般很少有人提及，但是他可以不用目标服务器配合，不过需要你搭建一个中转nginx服务器，用于转发请求。 nginx反向代理解决跨域 上面已经说到，禁止跨域问题其实是浏览器的一种安全行为，而现在的大多数解决方案都是用标签可以跨域访问的这个漏洞或者是技巧去完成，但都少不了目标服务器做相应的改变，而我最近遇到了一个需求是，目标服务器不能给予我一个header，更不可以改变代码返回个script，所以前5种方案都被我否决掉。最后因为我的网站是我自己的主机，所以我决定搭建一个nginx并把相应代码部署在它的下面，由页面请求本域名的一个地址，转由nginx代理处理后返回结果给页面，而且这一切都是同步的。假如www.a.com/html/msg.html想请求www.b.com/api/?method=1&amp;para=2。 www.a.com/html/msg.html如下： 123456var url = 'http://www.b.com/api/msg?method=1&amp;para=2'；&lt;br&gt;$.ajax(&#123;type: "GET",url:url,success: function(res)&#123;&#125;&#125;) 上面的请求必然会遇到跨域问题，这时我们需要修改一下我们的请求url，让请求发在nginx的一个url下： 123456var url = 'http://www.c.com/proxy/html/api/msg?method=1&amp;para=2'； //www.c.com是nginx主机地址 $.ajax(&#123;type: "GET",url:url,success: function(res)&#123;&#125;&#125;) nginx配置如下： 1234location ^~/proxy/html/&#123; rewrite ^/proxy/html/(.*)$ /$1 break; proxy_pass http://www.b.com/;&#125; ^~ /proxy/html/ 用于拦截请求，匹配任何以 /proxy/html/开头的地址，匹配符合以后，停止往下搜索正则。 rewrite ^/proxy/html/(.*)$ /$1 break; 代表重写拦截进来的请求，并且只能对域名后边的除去传递的参数外的字符串起作用，例如在www.c.com/proxy/html/api/msg?method=1&amp;para=2中只对/proxy/html/api/msg重写。 rewrite后面的参数是一个简单的正则 ^/proxy/html/(.*)$，$1代表正则中的第一个()，$2代表第二个()的值,以此类推。 break代表匹配一个之后停止匹配。 proxy_pass 既是把请求代理到其他主机。]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux创建目录或文件的默认权限]]></title>
    <url>%2F2017%2F09%2F15%2Flinux.umask%2F</url>
    <content type="text"><![CDATA[我们创建文件的默认权限是怎么来的？当我们登录系统之后创建一个文件总是有一个默认权限的，那么这个权限是怎么来的呢？这就是umask干的事情。umask设置了用户创建文件的默认权限，它与chmod的效果刚好相反，umask设置的是权限“补码”，而chmod设置的是文件权限码。 如何改变这个默认权限呢？ 一般在/etc/profile、$ [HOME]/.bash_profile或$[HOME]/.profile中设置umask值。 如何计算umask值？umask命令允许你设定文件创建时的缺省模式，对应每一类用户(文件属主、同组用户、其他用户)存在一个相应的umask值中的数字。对于文件来说，这一数字的最大值分别是6。系统不允许你在创建一个文本文件时就赋予它执行权限，必须在创建后用chmod命令增加这一权限。目录则允许设置执行权限，这样针对目录来说，umask中各个数字最大可以到7。 该命令的一般形式为：umask nnn其中nnn为umask置000 - 777 我们只要记住umask是从权限中“拿走”相应的位即可。下表是umask值与权限的对照表： umask file dir 0 6 7 1 6 6 2 4 5 3 4 4 4 2 3 5 2 2 6 0 1 7 0 0 如：umask值为022，则默认目录权限为755，默认文件权限为644。]]></content>
  </entry>
  <entry>
    <title><![CDATA[IO多路复用是什么意思]]></title>
    <url>%2F2017%2F09%2F15%2Fio.multiplexing%2F</url>
    <content type="text"><![CDATA[假设你是一个机场的空管， 你需要管理到你机场的所有的航线， 包括进港，出港， 有些航班需要放到停机坪等待，有些航班需要去登机口接乘客。你会怎么做?最简单的做法，就是你去招一大批空管员，然后每人盯一架飞机， 从进港，接客，排位，出港，航线监控，直至交接给下一个空港，全程监控。那么问题就来了： 很快你就发现空管塔里面聚集起来一大票的空管员，交通稍微繁忙一点，新的空管员就已经挤不进来了。 空管员之间需要协调，屋子里面就1, 2个人的时候还好，几十号人以后 ，基本上就成菜市场了。 空管员经常需要更新一些公用的东西，比如起飞显示屏，比如下一个小时后的出港排期，最后你会很惊奇的发现，每个人的时间最后都花在了抢这些资源上。 现实上我们的空管同时管几十架飞机稀松平常的事情， 他们怎么做的呢？他们用这个东西这个东西叫flight progress strip.每一个块代表一个航班，不同的槽代表不同的状态，然后一个空管员可以管理一组这样的块（一组航班），而他的工作，就是在航班信息有新的更新的时候，把对应的块放到不同的槽子里面。这个东西现在还没有淘汰哦，只是变成电子的了而已。是不是觉得一下子效率高了很多，一个空管塔里可以调度的航线可以是前一种方法的几倍到几十倍。如果你把每一个航线当成一个Sock(I/O 流), 空管当成你的服务端Sock管理代码的话.第一种方法就是最传统的多进程并发模型 (每进来一个新的I/O流会分配一个新的进程管理。)第二种方法就是I/O多路复用 (单个线程，通过记录跟踪每个I/O流(sock)的状态，来同时管理多个I/O流。) 其实“I/O多路复用”这个坑爹翻译可能是这个概念在中文里面如此难理解的原因。所谓的I/O多路复用在英文中其实叫 I/O multiplexing. 如果你搜索multiplexing啥意思，基本上都会出这个图：于是大部分人都直接联想到”一根网线，多个sock复用”这个概念，其实不管你用多进程还是I/O多路复用，网线都只有一根好伐。多个Sock复用一根网线这个功能是在内核＋驱动层实现的。 重要的事情再说一遍： I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态(对应空管塔里面的Fight progress strip槽)来同时管理多个I/O流.发明它的原因，是尽量多的提高服务器的吞吐能力。 是不是听起来好拗口，看个图就懂了.在同一个线程里面， 通过拨开关的方式，来同时传输多个I/O流， (学过EE的人现在可以站出来义正严辞说这个叫“时分复用”了）。 什么，你还没有搞懂“一个请求到来了，nginx使用epoll接收请求的过程是怎样的”， 多看看这个图就了解了。提醒下，ngnix会有很多链接进来， epoll会把他们都监视起来，然后像拨开关一样，谁有数据就拨向谁，然后调用相应的代码处理。]]></content>
  </entry>
  <entry>
    <title><![CDATA[修改系统时间]]></title>
    <url>%2F2017%2F09%2F14%2Flinux.modify.time%2F</url>
    <content type="text"><![CDATA[修改时区 1$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 看看时间是否正确若不正确就按如下操作 12$ date &#123;查看目前本地的时间&#125;$ hwclock --show &#123;查看硬件的时间&#125; 如果硬件时间和系统时间不同，那就对硬件的时间进行修改 123456$ #设置硬件时间为17年4月17日14点44分15秒$ hwclock --set --date '2017-04-17 14:44:15'$ #设置系统时间和硬件时间同步$ hwclock --hctosys$ #保存时钟$ clock -w]]></content>
  </entry>
  <entry>
    <title><![CDATA[short与long是限定符，不是基本数据类型]]></title>
    <url>%2F2017%2F09%2F14%2Fc.type%2F</url>
    <content type="text"><![CDATA[c语言只提供了几种基本数据类型 char int float double 而short，long为限定符，用来限定整型,12short int a;long int b; 我们平常只是出于习惯把int给省略了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux快速删除文件]]></title>
    <url>%2F2017%2F08%2F30%2Flinux.remove.file%2F</url>
    <content type="text"><![CDATA[使用rsync命令。这也是我被网上的一些资料坑的地方。 坑的过程如下： 第一次使用的命令1$ rsync --delete-before -a -H -v /root/test ./bak 其中，bak为要删除的目录，结果执行后，发现./bak目录下的文件没有删除，反而还将/root/test的空目录复制了过来。后来，多次验证下，才发现应该这么写：1$ rsync --delete-before -a -H -v /root/test/ ./bak]]></content>
  </entry>
  <entry>
    <title><![CDATA[Cache和Buffer的区别是什么]]></title>
    <url>%2F2017%2F08%2F05%2Fbuffer.cache%2F</url>
    <content type="text"><![CDATA[Buffer 简单说，Buffer的核心作用是用来缓冲，缓和冲击。比如你每秒要写100次硬盘，对系统冲击很大，浪费了大量时间在忙着处理开始写和结束写这两件事嘛。用个buffer暂存起来，变成每10秒写一次硬盘，对系统的冲击就很小，写入效率高了，日子过得爽了。极大缓和了冲击。 Cache Cache的核心作用是加快取用的速度。比如你一个很复杂的计算做完了，下次还要用结果，就把结果放手边一个好拿的地方存着，下次不用再算了。加快了数据取用的速度。 总结 所以，如果你注意关心过存储系统的话，你会发现硬盘的读写缓冲/缓存名称是不一样的，叫write-buffer和read-cache。很明显地说出了两者的区别。]]></content>
  </entry>
  <entry>
    <title><![CDATA[centos7以上添加端口]]></title>
    <url>%2F2017%2F08%2F02%2Fcentos.add.port%2F</url>
    <content type="text"><![CDATA[先查看firewalld是否开启 1$ systemctl status firewalld 要是没有开启则开启 1$ systemctl start firewalld 假设添加8000端口 1$ firewall-cmd --permanent --add-port=8000/tcp 重启firewalld 1$ systemctl restart firewalld 查看是否正确开启了端口 1$ firewall-cmd --query-port=8000/tcp firewalld开机自启 1$ systemctl enable firewalld]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux下使用vim修改jar包内文件内容]]></title>
    <url>%2F2017%2F08%2F01%2Flinux.vim%2F</url>
    <content type="text"><![CDATA[在平常工作中要经常把打包好的程序发布到linux机器中，有时候需要更改包内的文件，特别是对jar包中的配置文件进行更改后还要重新打包上传。当然也有别的替代方式。在这介绍一种通过vim命令不解压压缩包直接更改文件内容的方法，以log4j.xml文件为例： cd到jar包所在目录，运行命令 vim xxx.jar 1$ vim xxx.jar 输入如下查找命令查找文件 1/log4j.xml 光标移动到该文件上之后敲回车，进入编辑界面 更改文件后，使用vim命令保存并退出即可]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker + php + nginx + postgresql]]></title>
    <url>%2F2017%2F07%2F14%2Fdocker.php.nginx.postgresql%2F</url>
    <content type="text"><![CDATA[启动容器 123$ docker run --name gg.php -p 2120:2120 -p 2121:2121 -p 2123:2123 -d \-v /volume1/homes/ggzdttt/app/php:/var/www/html \php:7.1-fpm 如果步骤2和3都成功后执行下面的命令(记得将时间更改为北京时) 1234567891011121314$ docker exec -it gg.php /bin/bash$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime$ php /var/www/html/gzt/server/other/serial/realtime/start.php start -d$ exit$ docker run --name gg.nginx -p 8765:80 -d \-v /volume1/homes/ggzdttt/app/php:/usr/share/nginx/html \-v /volume1/homes/ggzdttt/nginx/conf.d:/etc/nginx/conf.d \--link gg.php:php \nginx$ docker exec -it gg.nginx /bin/bash$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime$ exit 安装php关于postgresql数据库的依赖 1234567$ docker exec -it gg.php /bin/bash$ apt-get update$ apt-get install -y libpq-dev$ /usr/local/bin/docker-php-ext-configure pgsql$ /usr/local/bin/docker-php-ext-install pgsql$ exit$ docker restart gg.php 安装workerman依赖 1234567891011121314151617$ docker exec -it gg.php /bin/bash$ apt-get install -y wget$ #执行`php -v`确定php版本，然后到http://php.net/releases/查找相应版本的php$ wget http://php.net/distributions/php-7.1.4.tar.gz$ tar zxvf php-7.1.4.tar.gz$ cd php-7.1.4/ext/pcntl/$ phpize$ ./configure$ make$ make install$ #通过运行 php --ini查找php.ini文件位置，然后在文件中添加extension=pcntl.so$ apt-get install -y vim$ vim /usr/local/etc/php/conf.d/docker-php-ext-pcntl.ini$ #在文件中添加extension=pcntl.so，最后保存退出$ php /var/www/html/gzt/server/other/serial/realtime/start.php start -d$ exit$ docker restart gg.php]]></content>
  </entry>
  <entry>
    <title><![CDATA[npm更换源]]></title>
    <url>%2F2017%2F07%2F12%2Fnode.npm%2F</url>
    <content type="text"><![CDATA[有很多方法来配置npm的registry地址，下面根据不同情境列出几种比较常用的方法。以淘宝npm 镜像举例： 临时使用 1$ npm --registry https://registry.npm.taobao.org install express 持久使用 1$ npm config set registry https://registry.npm.taobao.org 配置后可通过下面方式来验证是否成功1$ npm config get registry 或者1$ npm info express 通过cnpm使用，这也是我自己常使用的 1$ npm install -g cnpm --registry=https://registry.npm.taobao.org http://www.jianshu.com/p/0deb70e6f395]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux Shell 1>/dev/null 2>&1 含义]]></title>
    <url>%2F2017%2F07%2F07%2Flinux.pip%2F</url>
    <content type="text"><![CDATA[在shell中，每个进程都和三个系统文件 相关联：标准输入stdin，标准输出stdout、标准错误stderr，三个系统文件的文件描述符分别为0，1、2。所以这里2&gt;&amp;1 的意思就是将标准错误也输出到标准输出当中。 shell中可能经常能看到：echo log &gt; /dev/null 2&gt;&amp;1echo log &gt; /dev/null 2&gt;&amp;1的含义： /dev/null ：代表空设备文件 > ：代表重定向到哪里，例如：echo &quot;123&quot; &gt; /home/123.txt 0 ：表示stdin标准输入 1 ：表示stdout标准输出，系统默认值是1，所以”&gt;/dev/null”等同于”1&gt;/dev/null” 2 ：表示stderr标准错误 &amp; ：表示等同于的意思，2&gt;&amp;1，表示2的输出重定向等同于1 1 &gt; /dev/null 2&gt;&amp;1语句含义： 1 &gt; /dev/null ： 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息。 2&gt;&amp;1 ：接着，标准错误输出重定向（等同于）标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件。 cmd &gt;a 2&gt;a 和 cmd &gt;a 2&gt;&amp;1 为什么不同？ cmd &gt;a 2&gt;a ：stdout和stderr都直接送往文件 a ，a文件会被打开两遍，由此导致stdout和stderr互相覆盖。 cmd &gt;a 2&gt;&amp;1 ：stdout直接送往文件a ，stderr是继承了FD1的管道之后，再被送往文件a 。a文件只被打开一遍，就是FD1将其打开。 以上两者的不同点在于：cmd &gt;a 2&gt;a 相当于使用了FD1、FD2两个互相竞争使用文件 a 的管道。cmd &gt;a 2&gt;&amp;1 只使用了一个管道FD1，但已经包括了stdout和stderr。从IO效率上来讲，cmd &gt;a 2&gt;&amp;1的效率更高。 实例解析下面通过一个例子来展示2&gt;&amp;1有什么作用：123$ cat test.shtdate test.sh中包含两个命令，其中t是一个不存在的命令，执行会报错，默认情况下，错误会输出到stderr。date则能正确执行，并且输出时间信息，默认输出到stdout。1234$ ./test.sh &gt; test1.log./test.sh: line 1: t: command not found$ cat test1.logWed Jul 10 21:12:02 CST 2013 可以看到，date的执行结果被重定向到log文件中了，而t无法执行的错误则只打印在屏幕上。1234$ ./test.sh &gt; test2.log 2&gt;&amp;1$ cat test2.log./test.sh: line 1: t: command not foundTue Oct 9 20:53:44 CST 2007 这次，stderr和stdout的内容都被重定向到log文件中了。实际上， &gt; 就相当于 1&gt; 也就是重定向标准输出，不包括标准错误。通过2&gt;&amp;1，就将标准错误重定向到标准输出了，那么再使用&gt;重定向就会将标准输出和标准错误信息一同重定向了。如果只想重定向标准错误到文件中，则可以使用2&gt; file。 总结对于&amp;1 更准确的说应该是文件描述符 1，而1 一般代表的就是STDOUT_FILENO，实际上这个操作就是一个dup2(2)调用.他标准输出到all_result ，然后复制标准输出到文件描述符2(STDERR_FILENO)，其后果就是文件描述符1和2指向同一个文件表项,也可以说错误的输出被合并了，其中0 表示键盘输入 1表示屏幕输出 2表示错误输出，把标准出错重定向到标准输出,然后扔到/DEV/NULL下面去。通俗的说，就是把所有标准输出和标准出错都扔到垃圾桶里面。 command &gt;out.file 2&gt;&amp;1 &amp; command &gt;out.file是将command的输出重定向到out.file文件，即输出内容不打印到屏幕上，而是输出到out.file文件中。 2&gt;&amp;1 是将标准出错重定向到标准输出，这里的标准输出已经重定向到了out.file文件，即将标准出错也输出到out.file文件中。 最后一个&amp; ， 是让该命令在后台执行。 试想2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出，换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出. 可以试试 ls 2&gt;1不会报没有2文件的错误，但会输出一个空的文件1。 ls xxx 2&gt;1没有xxx这个文件的错误输出到了1中。 ls xxx 2&gt;&amp;1不会生成1这个文件了，不过错误跑到标准输出了。 ls xxx &gt;out.txt 2&gt;&amp;1实际上可换成ls xxx 1&gt;out.txt 2&gt;&amp;1，重定向符号&gt;默认是1,错误和输出都传到out.txt了。 为何2&gt;&amp;1要写在后面？ command &gt; file 2&gt;&amp;1首先是command &gt; file将标准输出重定向到file中， 2&gt;&amp;1 是标准错误拷贝了标准输出的行为，也就是同样被重定向到file中，最终结果就是标准输出和错误都被重定向到file中。 command 2&gt;&amp;1 &gt;file2&gt;&amp;1 标准错误拷贝了标准输出的行为，但此时标准输出还是在终端。&gt;file 后输出才被重定向到file，但标准错误仍然保持在终端。]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux内存分析]]></title>
    <url>%2F2017%2F06%2F23%2Flinux.memory%2F</url>
    <content type="text"><![CDATA[分析top命令以及smaps讲得比较浅显易懂link 如何设置swap大小根据centos官网介绍可以得出如下公式：M = Amount of RAM in GB, and S = Amount of swap in GB, then If M &lt; 2, S = M *2 Else S = M + 2。而且其最小不应该小于32M(never less than 32 MB.)link linux内存介绍以及C与C++内存管理可以关注一下这个人的其他博文link]]></content>
  </entry>
  <entry>
    <title><![CDATA[后台运行]]></title>
    <url>%2F2017%2F06%2F22%2Fnohup%2F</url>
    <content type="text"><![CDATA[$ java -jar xx.jar当前ssh窗口被锁定，可按CTRL + C打断程序运行，或直接关闭窗口，程序退出。 $ java -jar xx.jar &amp;窗口不锁定，&amp;代表在后台运行,当前ssh窗口不被锁定，但是当窗口关闭时，程序中止运行。 $ nohup java -jar xx.jar &amp;窗口不锁定，窗口关闭时，程序仍然运行，nohup意思是不挂断运行命令,当账户退出或终端关闭时,程序仍然运行。当用nohup命令执行作业时，缺省情况下该作业的所有输出被重定向到nohup.out的文件中，除非另外指定了输出文件。$ nohup java -jar xx.jar &gt; temp.log &amp; 可通过jobs命令查看后台运行任务，执行jobs命令后，就会列出所有后台执行的作业，并且每个作业前面都有个编号。如果想将某个作业调回前台控制，只需要fg + 编号即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[centos挂载ftp目录]]></title>
    <url>%2F2017%2F06%2F15%2Fmount.ftp%2F</url>
    <content type="text"><![CDATA[安装rpmforge-releae去某个地址)下载rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm，然后执行命令rpm -Uhv rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm 安装curlftpfs$ yum install curlftpfs 挂载将挂载后的地址先创建好，例： 12$ mkdir -p /mod/t639/gmf$ curlftpfs -o rw,allow_other ftp://nwpdata:123456@10.20.49.99/mod/t639/gmf /mod/t639/gmf 卸载$ fusermount -u /mod/t639/gmf]]></content>
  </entry>
  <entry>
    <title><![CDATA[查找对应目录进行压缩]]></title>
    <url>%2F2017%2F06%2F08%2Fdirectory.tar%2F</url>
    <content type="text"><![CDATA[$ tar zcvf 20170608.tar.gz $(find ./post_data/png/ -name &quot;20170608&quot; -type d)]]></content>
  </entry>
  <entry>
    <title><![CDATA[查看系统开机时间]]></title>
    <url>%2F2017%2F06%2F01%2Fuptime%2F</url>
    <content type="text"><![CDATA[复杂的命令$ date -d &quot;$(awk -F. &#39;{print $1}&#39; /proc/uptime) second ago&quot; +&quot;%Y-%m-%d %H:%M:%S&quot; 简单的命令执行top，左上角就是开机时间]]></content>
  </entry>
  <entry>
    <title><![CDATA[valgrind命令]]></title>
    <url>%2F2017%2F05%2F24%2Fvalgrind%2F</url>
    <content type="text"><![CDATA[$ valgrind --leak-check=full --show-reachable=yes --log-file=bump.log ./bump]]></content>
  </entry>
  <entry>
    <title><![CDATA[nginx相关的一些命令]]></title>
    <url>%2F2017%2F05%2F20%2Fnginx%2F</url>
    <content type="text"><![CDATA[检查nginx的配置$ nginx -t 启动nginx$ nginx -c /usr/local/nginx/conf/nginx.confconf位置得自己找找 重启nginx$ nginx -s reload 启动cgi$ spawn-fcgi -a 127.0.0.1 -p 9002 -C 25 -f /home/zhengdongtian/CLionProjects/Bump/cmake-build-debug/Bump9002是对应的conf中的端口]]></content>
  </entry>
  <entry>
    <title><![CDATA[关闭selinux]]></title>
    <url>%2F2017%2F05%2F19%2Fselinux%2F</url>
    <content type="text"><![CDATA[永久关闭selinux将/etc/sysconfig/selinux和/etc/selinux/config中的SELINUX=enforcing改为SELINUX=disabled 临时关闭selinux$ setenforce 0]]></content>
  </entry>
  <entry>
    <title><![CDATA[创建本地yum源]]></title>
    <url>%2F2017%2F05%2F19%2Flocal.repo%2F</url>
    <content type="text"><![CDATA[在没有网络，但是拥有系统光盘或者系统u盘的情况下，可以创建本地yum源来安装一些软件。 以centos为例： 挂载光盘或u盘 12$ mkdir -p /yum/system$ mount /dev/hdc /yum/system 上面的/dev/hdc是插入光盘或者u盘后的路径，通过disk -l查看 创建local.repo，并添加内容，相关内容如下 123456$ cat /etc/yum.repos.d/local.repo[base] name=localbaseurl=file:///yum/systemgpgcheck=0enabled=1]]></content>
  </entry>
  <entry>
    <title><![CDATA[向环境中添加动态库路径]]></title>
    <url>%2F2017%2F05%2F19%2Fld.so.conf%2F</url>
    <content type="text"><![CDATA[如果想改变LD_LIBRARY_PATH的值，如下操作： 在/etc/ld.so.conf中添加你的path 执行ldconfig]]></content>
  </entry>
  <entry>
    <title><![CDATA[增加swap大小]]></title>
    <url>%2F2017%2F04%2F30%2Fswap%2F</url>
    <content type="text"><![CDATA[增加swap大小, 30G 左右 1$ dd if=/dev/zero of=/public/swap bs=1024 count=30720000 设置交换文件 1$ mkswap /public/swap 立即激活启用交换分区 1$ swapon /public/swap 添加系统引导时自启动运行 打开/etc/fstab 1$ vi /etc/fstab 将光标移动至最后一行按o键（意思是在光标行的下面添加一行） 1/public/swap swap swap defaults 0 0 退出保存，先按退出键，在敲:wq，最后回车]]></content>
  </entry>
</search>
